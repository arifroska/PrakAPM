{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Decision Tree ID3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arifroska/PrakAPM/blob/main/Decision_Tree_ID3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GadId3Classifier:\n",
        "  def fit(self, input, output):\n",
        "    data = input.copy()\n",
        "    data[output.name] = output\n",
        "    self.tree = self.decision_tree(data, data, input.columns, output.name)\n",
        "\n",
        "  def predict(self, input):\n",
        "    # mengubah data input menjadi kamus sampel\n",
        "    samples = input.to_dict(orient='records')\n",
        "    predictions = []\n",
        "\n",
        "    # buat prediksi untuk setiap sampel\n",
        "    for sample in samples:\n",
        "      predictions.append(self.make_prediction(sample, self.tree, 1.0))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "  def entropy(self, attribute_column):\n",
        "    # temukan nilai unik dan frekuensinya dihitung untuk atribut yang diberikan\n",
        "    values, counts = np.unique(attribute_column, return_counts=True)\n",
        "\n",
        "    # hitung entropi untuk setiap nilai unik\n",
        "    entropy_list = []\n",
        "\n",
        "    for i in range(len(values)):\n",
        "      probability = counts[i]/np.sum(counts)\n",
        "      entropy_list.append(-probability*np.log2(probability))\n",
        "\n",
        "    # hitung jumlah nilai entropi individu\n",
        "    total_entropy = np.sum(entropy_list)\n",
        "\n",
        "    return total_entropy\n",
        "\n",
        "  def information_gain(self, data, feature_attribute_name, target_attribute_name):\n",
        "    # temukan entropi total dari subset yang diberikan\n",
        "    total_entropy = self.entropy(data[target_attribute_name])\n",
        "\n",
        "    # temukan nilai unik dan frekuensinya dihitung untuk atribut yang akan dipisah\n",
        "    values, counts = np.unique(data[feature_attribute_name], return_counts=True)\n",
        "\n",
        "    # hitung entropi tertimbang dari subset\n",
        "    weighted_entropy_list = []\n",
        "\n",
        "    for i in range(len(values)):\n",
        "      subset_probability = counts[i]/np.sum(counts)\n",
        "      subset_entropy = self.entropy(data.where(data[feature_attribute_name]==values[i]).dropna()[target_attribute_name])\n",
        "      weighted_entropy_list.append(subset_probability*subset_entropy)\n",
        "\n",
        "    total_weighted_entropy = np.sum(weighted_entropy_list)\n",
        "\n",
        "    # menghitung perolehan informasi\n",
        "    information_gain = total_entropy - total_weighted_entropy\n",
        "\n",
        "    return information_gain\n",
        "\n",
        "  def decision_tree(self, data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class=None):\n",
        "    # base cases:\n",
        "    # jika datanya murni, kembalikan kelas mayoritas dari subset\n",
        "    unique_classes = np.unique(data[target_attribute_name])\n",
        "    if len(unique_classes) <= 1:\n",
        "      return unique_classes[0]\n",
        "    # jika subset kosong, mis. tidak ada sampel, kembalikan sebagian besar kelas data asli\n",
        "    elif len(data) == 0:\n",
        "      majority_class_index = np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])\n",
        "      return np.unique(original_data[target_attribute_name])[majority_class_index]\n",
        "    # jika kumpulan data tidak berisi fitur untuk dilatih, kembalikan kelas simpul induk\n",
        "    elif len(feature_attribute_names) == 0:\n",
        "      return parent_node_class\n",
        "    # jika tidak satu pun di atas yang benar, buat cabang:\n",
        "    else:\n",
        "      # tentukan kelas simpul induk dari cabang saat ini\n",
        "      majority_class_index = np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])\n",
        "      parent_node_class = unique_classes[majority_class_index]\n",
        "\n",
        "      # tentukan nilai perolehan informasi untuk setiap fitur\n",
        "      # pilih fitur yang paling baik membagi data, mis. nilai tertinggi\n",
        "      ig_values = [self.information_gain(data, feature, target_attribute_name) for feature in feature_attribute_names]\n",
        "      best_feature_index = np.argmax(ig_values)\n",
        "      best_feature = feature_attribute_names[best_feature_index]\n",
        "\n",
        "      # buat struktur pohon, kosongkan terlebih dahulu\n",
        "      tree = {best_feature: {}}\n",
        "\n",
        "      # hapus fitur terbaik dari fitur yang tersedia, itu akan menjadi simpul induk\n",
        "      feature_attribute_names = [i for i in feature_attribute_names if i != best_feature]\n",
        "\n",
        "      #buat node di bawah node induk\n",
        "      parent_attribute_values = np.unique(data[best_feature])\n",
        "      for value in parent_attribute_values:\n",
        "        sub_data = data.where(data[best_feature] == value).dropna()\n",
        "\n",
        "        # panggil algoritma secara rekursif\n",
        "        subtree = self.decision_tree(sub_data, orginal_data, feature_attribute_names, target_attribute_name, parent_node_class)\n",
        "\n",
        "        # tambahkan subpohon ke pohon asli\n",
        "        tree[best_feature][value] = subtree\n",
        "\n",
        "      return tree\n",
        "\n",
        "  def make_prediction(self, sample, tree, default=1):\n",
        "    # memetakan data sampel ke pohon\n",
        "    for attribute in list(sample.keys()):\n",
        "      # periksa apakah fitur ada di pohon\n",
        "      if attribute in list(tree.keys()):\n",
        "        try:\n",
        "          result = tree[attribute][sample[attribute]]\n",
        "        except:\n",
        "          return default\n",
        "\n",
        "        result = tree[attribute][sample[attribute]]\n",
        "\n",
        "        # jika ada lebih banyak atribut dalam hasil, temukan hasil terbaik secara rekursif\n",
        "        if isinstance(result, dict):\n",
        "          return self.make_prediction(sample, result)\n",
        "        else:\n",
        "          return result"
      ],
      "metadata": {
        "id": "8JTD_MnNsQEr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLGaOj3Mr_iF",
        "outputId": "8fb6ede7-b11b-423e-ab77-abc7a47dd420"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5555555555555556"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "df = pd.read_csv(data_url, header=None)\n",
        "\n",
        "# ganti nama kolom yang dikenal\n",
        "columns = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "           'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'disease_present']\n",
        "df.columns = columns\n",
        "\n",
        "# ubah fitur penyakit sekarang menjadi biner\n",
        "df['disease_present'] = df.disease_present.replace([1,2,3,4], 1)\n",
        "\n",
        "# jatuhkan baris dengan nilai yang hilang, missing = ?\n",
        "df = df.replace(\"?\", np.nan)\n",
        "df = df.dropna()\n",
        "\n",
        "# mengatur data menjadi input dan output\n",
        "X = df.drop(columns=\"disease_present\")\n",
        "y = df[\"disease_present\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# inisialisasi dan sesuaikan model\n",
        "model = GadId3Classifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# kembali skor akurasi\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMhOv8WvF2X7",
        "outputId": "232805bd-0839-40f2-b382-df608ddfc4f7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
            "0    63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
            "1    67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
            "2    67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
            "3    37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
            "4    41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
            "..    ...  ...  ...       ...    ...  ...      ...      ...    ...      ...   \n",
            "297  57.0  0.0  4.0     140.0  241.0  0.0      0.0    123.0    1.0      0.2   \n",
            "298  45.0  1.0  1.0     110.0  264.0  0.0      0.0    132.0    0.0      1.2   \n",
            "299  68.0  1.0  4.0     144.0  193.0  1.0      0.0    141.0    0.0      3.4   \n",
            "300  57.0  1.0  4.0     130.0  131.0  0.0      0.0    115.0    1.0      1.2   \n",
            "301  57.0  0.0  2.0     130.0  236.0  0.0      2.0    174.0    0.0      0.0   \n",
            "\n",
            "     slope   ca thal  disease_present  \n",
            "0      3.0  0.0  6.0                0  \n",
            "1      2.0  3.0  3.0                1  \n",
            "2      2.0  2.0  7.0                1  \n",
            "3      3.0  0.0  3.0                0  \n",
            "4      1.0  0.0  3.0                0  \n",
            "..     ...  ...  ...              ...  \n",
            "297    2.0  0.0  7.0                1  \n",
            "298    2.0  0.0  7.0                1  \n",
            "299    2.0  2.0  7.0                1  \n",
            "300    2.0  1.0  7.0                1  \n",
            "301    2.0  1.0  3.0                1  \n",
            "\n",
            "[297 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.tree import export_graphviz\n",
        "# from six import StringIO  \n",
        "# from IPython.display import Image  \n",
        "# import pydotplus\n",
        "\n",
        "# dot_data = StringIO()\n",
        "# export_graphviz(model, out_file=dot_data,  \n",
        "#                 filled=True, rounded=True,\n",
        "#                 special_characters=True,feature_names = model,class_names=['Tidak Sakit Dada','Sakit Dada'])\n",
        "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "# graph.write_png('Lung Cancer.png')\n",
        "# Image(graph.create_png())"
      ],
      "metadata": {
        "id": "ZTxdDO9K1WK8"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}